{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adast/TransResUNet/blob/master/TransResUNet_b16_fullres_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQVpdWCYV0em"
      },
      "source": [
        "## Install necessary libraries, download dataset, download model, download pre trained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeryIUmK64ww"
      },
      "outputs": [],
      "source": [
        "! pip install transformers\n",
        "! pip install ml_collections\n",
        "! pip install torchinfo\n",
        "\n",
        "! 7z x \"/content/drive/MyDrive/Hand Segmentation/train.zip\" -o./dataset/train '-xr!__MACOSX'\n",
        "! 7z x \"/content/drive/MyDrive/Hand Segmentation/test.zip\" -o./dataset/test '-xr!__MACOSX'\n",
        "! mv dataset/train/train/* dataset/train/ && rm -rf dataset/train/train\n",
        "! mv dataset/test/test/* dataset/test/ && rm -rf dataset/test/test\n",
        "! cp \"/content/drive/MyDrive/Hand Segmentation/sample_submission.csv\" sample_submission.csv\n",
        "\n",
        "! git clone https://github.com/adast/TransResUNet\n",
        "! wget https://storage.googleapis.com/vit_models/imagenet21k/R50%2BViT-B_16.npz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcyWJ7e0WUh5"
      },
      "source": [
        "## Import libraries, set manaul seed, utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Fb_zTYTw61YC"
      },
      "outputs": [],
      "source": [
        "# TransUnet\n",
        "import sys\n",
        "sys.path.insert(0, './TransResUNet')\n",
        "from TransResUNet.models.trans_resunet import TransResUNet\n",
        "\n",
        "# Transformers\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms\n",
        "from torchinfo import summary\n",
        "\n",
        "# Others\n",
        "import ml_collections\n",
        "import os\n",
        "import glob\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from tqdm.notebook import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "# Make computations repeatable\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "# Compute on gpu if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Run length encoding\n",
        "def rle_encoding(x):\n",
        "    '''\n",
        "    x: numpy array of shape (height, width), 1 - mask, 0 - background\n",
        "    Returns run length as list\n",
        "    '''\n",
        "    dots = np.where(x.T.flatten()==1)[0] # .T sets Fortran order down-then-right\n",
        "    run_lengths = []\n",
        "    prev = -2\n",
        "    for b in dots:\n",
        "        if (b>prev+1): run_lengths.extend((b+1, 0))\n",
        "        run_lengths[-1] += 1\n",
        "        prev = b\n",
        "    return run_lengths\n",
        "\n",
        "# Dice score\n",
        "def dice_score(y_true, y_pred):\n",
        "    return torch.sum(y_pred[y_true==1])*2.0 / (torch.sum(y_pred) + torch.sum(y_true))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOSW-ycMWogw"
      },
      "source": [
        "## Define dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Zn6lceuz7BkV"
      },
      "outputs": [],
      "source": [
        "class HandSegmentationDataset(Dataset):\n",
        "    def __init__(self, path: str):\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "        self.normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "\n",
        "        self.paths = []\n",
        "        folders = sorted([os.path.join(path, f) for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))])\n",
        "        for folder in folders:\n",
        "            image_paths = sorted(glob.glob(f'{folder}/images/*.png'))\n",
        "            segmentation_paths = sorted(glob.glob(f'{folder}/segmentation/*.png'))\n",
        "            for image_path, segmentation_path in zip(image_paths, segmentation_paths):\n",
        "                assert image_path != segmentation_path\n",
        "                self.paths.append((image_path, segmentation_path))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        image = Image.open(self.paths[index][0]).convert('RGB')\n",
        "        pixel_values = self.to_tensor(image)\n",
        "        pixel_values = self.normalize(pixel_values)\n",
        "\n",
        "        segmentation = Image.open(self.paths[index][1]).convert('L')\n",
        "        segmentation = self.to_tensor(segmentation)\n",
        "        segmentation = (segmentation > 0.1).float()\n",
        "\n",
        "        return {\n",
        "            'pixel_values': torch.squeeze(pixel_values),\n",
        "            'segmentations': segmentation\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuFmH5wvWvtw"
      },
      "source": [
        "## Initialize dataset and dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "e65U9gEdBQCH"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 3\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "# Create and split dataset to train and val\n",
        "dataset = HandSegmentationDataset('dataset/train/')\n",
        "train_size = int(len(dataset) * 0.8)\n",
        "val_size = len(dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Create dataloaders\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kcwnBIQXX0A"
      },
      "source": [
        "## Define hyperparams. Initialize model, optimizer, scheduler, criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzRrvBeo7S06",
        "outputId": "7fc00591-0dab-4ff1-aa40-d90275655be5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Resized position embedding: torch.Size([1, 197, 768]) to torch.Size([1, 1601, 768])\n",
            "Position embedding grid-size from [14, 14] to [40, 40]\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 30\n",
        "LEARNING_RATE = 5e-5\n",
        "\n",
        "def get_r50_b16_config():\n",
        "    config = ml_collections.ConfigDict()\n",
        "    \n",
        "    config.image_size = (480, 640)\n",
        "    config.n_classes = 1\n",
        "    config.pre_trained_path = 'R50+ViT-B_16.npz'\n",
        "    \n",
        "    config.resnet = ml_collections.ConfigDict()\n",
        "    # Using three bottleneck blocks results in a downscaling of 2^(1 + 3)=16 which\n",
        "    # results in an effective patch size of /16.\n",
        "    config.resnet.num_layers = (3, 4, 9)\n",
        "    config.resnet.width_factor = 1\n",
        "    \n",
        "    config.transformer = ml_collections.ConfigDict()\n",
        "    config.transformer.num_special_tokens = 1\n",
        "    config.transformer.patch_size = 16\n",
        "    config.transformer.hidden_size = 768\n",
        "    config.transformer.mlp_dim = 3072\n",
        "    config.transformer.num_heads = 12\n",
        "    config.transformer.num_layers = 12\n",
        "    config.transformer.attention_dropout_rate = 0.0\n",
        "    config.transformer.dropout_rate = 0.1\n",
        "    \n",
        "    config.decoder = ml_collections.ConfigDict()\n",
        "    config.decoder.head_channels = 512\n",
        "    \n",
        "    return config\n",
        "\n",
        "def get_r50_l32_config():\n",
        "    \"\"\"Returns the ViT-L/32 configuration.\"\"\"\n",
        "    config = ml_collections.ConfigDict()\n",
        "    \n",
        "    config.image_size = (480, 640)\n",
        "    config.n_classes = 1\n",
        "    config.pre_trained_path = 'R50+ViT-L_32.npz'\n",
        "    \n",
        "    config.resnet = ml_collections.ConfigDict()\n",
        "    # Using four bottleneck blocks results in a downscaling of 2^(1 + 4)=32 which\n",
        "    # results in an effective patch size of /32.\n",
        "    config.resnet.num_layers = (3, 4, 6, 3)\n",
        "    config.resnet.width_factor = 1\n",
        "    \n",
        "    config.transformer = ml_collections.ConfigDict()\n",
        "    config.transformer.num_special_tokens = 1\n",
        "    config.transformer.patch_size = 32\n",
        "    config.transformer.hidden_size = 1024\n",
        "    config.transformer.mlp_dim = 4096\n",
        "    config.transformer.num_heads = 16\n",
        "    config.transformer.num_layers = 24\n",
        "    config.transformer.attention_dropout_rate = 0.0\n",
        "    config.transformer.dropout_rate = 0.1\n",
        "    \n",
        "    config.decoder = ml_collections.ConfigDict()\n",
        "    config.decoder.head_channels = 512\n",
        "    \n",
        "    return config\n",
        "\n",
        "config = get_r50_b16_config()\n",
        "model = TransResUNet(config)\n",
        "model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
        "total_steps = len(train_dataloader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer, \n",
        "    num_warmup_steps = len(train_dataloader),\n",
        "    num_training_steps = total_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EYYfMkCYeQc"
      },
      "source": [
        "## Show model structure and info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz9Px62E_4Tq",
        "outputId": "a561c2d1-ca9f-4b9f-d89c-8ae6aae896db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "=====================================================================================\n",
              "Layer (type:depth-idx)                                       Param #\n",
              "=====================================================================================\n",
              "TransResUNet                                                 --\n",
              "├─HybridVit: 1-1                                             --\n",
              "│    └─Embeddings: 2-1                                       --\n",
              "│    │    └─ResNetV2: 3-1                                    11,894,848\n",
              "│    │    └─Conv2d: 3-2                                      787,200\n",
              "│    │    └─Dropout: 3-3                                     --\n",
              "│    └─Encoder: 2-2                                          --\n",
              "│    │    └─ModuleList: 3-4                                  85,054,464\n",
              "│    │    └─LayerNorm: 3-5                                   1,536\n",
              "├─Conv2dReLU: 1-2                                            --\n",
              "│    └─Conv2d: 2-3                                           3,538,944\n",
              "│    └─BatchNorm2d: 2-4                                      1,024\n",
              "│    └─ReLU: 2-5                                             --\n",
              "├─ModuleList: 1-3                                            --\n",
              "│    └─ResDecoderBlock: 2-6                                  --\n",
              "│    │    └─ConvTranspose2d: 3-6                             1,049,088\n",
              "│    │    └─ResConv: 3-7                                     5,312,256\n",
              "│    └─ResDecoderBlock: 2-7                                  --\n",
              "│    │    └─ConvTranspose2d: 3-8                             262,400\n",
              "│    │    └─ResConv: 3-9                                     1,329,024\n",
              "│    └─ResDecoderBlock: 2-8                                  --\n",
              "│    │    └─ConvTranspose2d: 3-10                            65,664\n",
              "│    │    └─ResConv: 3-11                                    258,880\n",
              "│    └─ResDecoderBlock: 2-9                                  --\n",
              "│    │    └─ConvTranspose2d: 3-12                            16,448\n",
              "│    │    └─ResConv: 3-13                                    46,432\n",
              "├─Conv2d: 1-4                                                33\n",
              "=====================================================================================\n",
              "Total params: 109,618,241\n",
              "Trainable params: 109,618,241\n",
              "Non-trainable params: 0\n",
              "====================================================================================="
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "summary(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-VyGCHfYuZ8"
      },
      "source": [
        "## Train and validation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "e73c0rmH8Gza"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, loss_fn, optimizer, scheduler, device, writer=None, epoch_index=0):\n",
        "    # Tracking variables.\n",
        "    losses = []\n",
        "\n",
        "    # Put the model into training mode.\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for batch_index, batch in enumerate(tqdm(dataloader, total=len(dataloader), desc=\"Training on batches\")):\n",
        "        global_batch_index = epoch_index * len(dataloader) + batch_index # Global step index\n",
        "\n",
        "        pixel_values = batch['pixel_values'].to(device) # Pixel values\n",
        "        segmentations = batch['segmentations'].to(device) # Segmentation\n",
        "        \n",
        "        # Forward\n",
        "        outputs = model(pixel_values)\n",
        "        loss = loss_fn(outputs, segmentations)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Clip the norm of gradient to prevent gradient expolosion\n",
        "        optimizer.step() # Update weights\n",
        "        scheduler.step() # Update the learning rate.\n",
        "\n",
        "        # Write loss per batch to tensorboard\n",
        "        if writer is not None:\n",
        "            writer.add_scalar('Loss/train (per batch)', loss.item(), global_batch_index)\n",
        "\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "def val_epoch(model, dataloader, loss_fn, device, writer=None, epoch_index=0):\n",
        "    # Tracking variables.\n",
        "    losses = []\n",
        "    metrics = []\n",
        "\n",
        "    # Put the model into evaluation mode.\n",
        "    model.eval()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    with torch.no_grad():\n",
        "        for batch_index, batch in enumerate(tqdm(dataloader, total=len(dataloader), desc=\"Validation on batches\")):\n",
        "            global_batch_index = epoch_index * len(dataloader) + batch_index # Global step index\n",
        "\n",
        "            pixel_values = batch['pixel_values'].to(device) # Pixel values\n",
        "            segmentations = batch['segmentations'].to(device) # Segmentation\n",
        "            \n",
        "            # Forward\n",
        "            outputs = model(pixel_values)\n",
        "            loss = loss_fn(outputs, segmentations)\n",
        "\n",
        "            # Compute metric\n",
        "            outputs = torch.sigmoid(outputs)\n",
        "            outputs = (outputs > 0.5).float()\n",
        "            metric = dice_score(segmentations, outputs)\n",
        "\n",
        "            losses.append(loss.item())\n",
        "            metrics.append(metric.item())\n",
        "\n",
        "            # Write to tensorboard\n",
        "            if writer is not None:\n",
        "                writer.add_scalar('Loss/val (per batch)', loss.item(), global_batch_index)\n",
        "                writer.add_scalar('Dice/val (per batch)', metric.item(), global_batch_index)\n",
        "\n",
        "    return np.mean(losses), np.mean(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5Fi5lxpY7-r"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hig3oNkUDEP7"
      },
      "outputs": [],
      "source": [
        "TENSORBOARD_DIR = '/content/drive/MyDrive/rucode_2021/segmentation/transunet-r50-b16-fullres/tensorboard'\n",
        "CHECKPOINTS_DIR = '/content/drive/MyDrive/rucode_2021/segmentation/transunet-r50-b16-fullres/checkpoints'\n",
        "! mkdir -p {CHECKPOINTS_DIR}\n",
        "\n",
        "# Tensorboard\n",
        "writer = SummaryWriter(log_dir=TENSORBOARD_DIR)\n",
        "\n",
        "# Loop through each epoch.\n",
        "for epoch in tqdm(range(EPOCHS), desc=\"Epoch\"):\n",
        "    # Perform one full pass over the training and validation sets\n",
        "    train_loss = train_epoch(model, train_dataloader, criterion, optimizer, scheduler, device, writer, epoch)\n",
        "    val_loss, val_metric = val_epoch(model, val_dataloader, criterion, device, writer, epoch)\n",
        "\n",
        "    # Populate tensorboard\n",
        "    writer.add_scalar('Loss/train (per epoch)', train_loss, epoch)\n",
        "    writer.add_scalar('Loss/val (per epoch)',val_loss, epoch)\n",
        "    writer.add_scalar('Dice/val (per epoch)',val_metric, epoch)\n",
        "\n",
        "    # Print loss and accuracy values to see how training evolves.\n",
        "    print(f'train_loss: {train_loss:.5f} - val_loss: {val_loss:.5f} - dice: {val_metric:.5f}\\n')\n",
        "\n",
        "    # Save checkpoint\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict()\n",
        "    }, f\"{CHECKPOINTS_DIR}/epoch-{epoch}_vl_{val_loss:.5f}_dice_{val_metric:.5f}.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lqvJRJ84Tn2O"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyM0yDcGd1MoVVLAHWfKuOIQ",
      "collapsed_sections": [],
      "include_colab_link": true,
      "mount_file_id": "1W97n3BTP1rvMjCNlgCC8TbSzvXtPW9cw",
      "name": "TransResUNet b16 fullres training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
