{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eb3a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huggingface\n",
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import transforms\n",
    "\n",
    "# Visualization libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "# Others\n",
    "from typing import Tuple\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Logging\n",
    "from importlib import reload\n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%I:%M:%S')\n",
    "\n",
    "# Config visualization output\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "sns.set_palette(sns.color_palette([\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]))\n",
    "\n",
    "# Make computations repeatable\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# Compute on gpu if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Run length encoding\n",
    "def rle_encoding(x):\n",
    "    '''\n",
    "    x: numpy array of shape (height, width), 1 - mask, 0 - background\n",
    "    Returns run length as list\n",
    "    '''\n",
    "    dots = np.where(x.T.flatten()==1)[0] # .T sets Fortran order down-then-right\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b>prev+1): run_lengths.extend((b+1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "# Dice score\n",
    "def dice_score(y_true, y_pred):\n",
    "    return torch.sum(y_pred[y_true==1])*2.0 / (torch.sum(y_pred) + torch.sum(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8e6cb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.trans_resunet import TransResUNet\n",
    "import ml_collections\n",
    "from torchinfo import summary\n",
    "\n",
    "def get_r50_b16_config():\n",
    "    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n",
    "    config = ml_collections.ConfigDict()\n",
    "    \n",
    "    config.image_size = (640, 480)\n",
    "    config.n_classes = 1\n",
    "    config.pre_trained_path = 'imagenet21k_R50+ViT-B_16.npz'\n",
    "    \n",
    "    config.resnet = ml_collections.ConfigDict()\n",
    "    # Using three bottleneck blocks results in a downscaling of 2^(1 + 3)=16 which\n",
    "    # results in an effective patch size of /16.\n",
    "    config.resnet.num_layers = (3, 4, 9)\n",
    "    config.resnet.width_factor = 1\n",
    "    \n",
    "    config.transformer = ml_collections.ConfigDict()\n",
    "    config.transformer.num_special_tokens = 1\n",
    "    config.transformer.patch_size = 16\n",
    "    config.transformer.hidden_size = 768\n",
    "    config.transformer.mlp_dim = 3072\n",
    "    config.transformer.num_heads = 12\n",
    "    config.transformer.num_layers = 12\n",
    "    config.transformer.attention_dropout_rate = 0.0\n",
    "    config.transformer.dropout_rate = 0.1\n",
    "    \n",
    "    return config\n",
    "\n",
    "def get_r50_l32_config():\n",
    "    \"\"\"Returns the ViT-L/32 configuration.\"\"\"\n",
    "    config = ml_collections.ConfigDict()\n",
    "    \n",
    "    config.image_size = (640, 480)\n",
    "    config.n_classes = 1\n",
    "    config.pre_trained_path = 'imagenet21k_R50+ViT-L_32.npz'\n",
    "    \n",
    "    config.resnet = ml_collections.ConfigDict()\n",
    "    # Using four bottleneck blocks results in a downscaling of 2^(1 + 4)=32 which\n",
    "    # results in an effective patch size of /32.\n",
    "    config.resnet.num_layers = (3, 4, 6, 3)\n",
    "    config.resnet.width_factor = 1\n",
    "    \n",
    "    config.transformer = ml_collections.ConfigDict()\n",
    "    config.transformer.num_special_tokens = 1\n",
    "    config.transformer.patch_size = 32\n",
    "    config.transformer.hidden_size = 1024\n",
    "    config.transformer.mlp_dim = 4096\n",
    "    config.transformer.num_heads = 16\n",
    "    config.transformer.num_layers = 24\n",
    "    config.transformer.attention_dropout_rate = 0.0\n",
    "    config.transformer.dropout_rate = 0.1\n",
    "    \n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbc559d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resized position embedding: torch.Size([1, 50, 1024]) to torch.Size([1, 401, 1024])\n",
      "Position embedding grid-size from [7, 7] to [20, 20]\n"
     ]
    }
   ],
   "source": [
    "config = get_r50_l32_config()\n",
    "# config = get_r50_b16_config()\n",
    "net = TransResUNet(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f650421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================================\n",
       "Layer (type:depth-idx)                                       Output Shape              Param #\n",
       "==============================================================================================================\n",
       "TransResUNet                                                 --                        --\n",
       "├─ModuleList: 1-1                                            --                        --\n",
       "├─HybridVit: 1-2                                             [1, 301, 1024]            --\n",
       "│    └─Embeddings: 2-1                                       [1, 301, 1024]            26,017,856\n",
       "│    └─Encoder: 2-2                                          [1, 301, 1024]            302,311,424\n",
       "├─ModuleList: 1-1                                            --                        --\n",
       "│    └─ResDecoderBlock: 2-3                                  [1, 512, 40, 30]          21,241,344\n",
       "│    └─ResDecoderBlock: 2-4                                  [1, 256, 80, 60]          5,312,256\n",
       "│    └─ResDecoderBlock: 2-5                                  [1, 128, 160, 120]        1,329,024\n",
       "│    └─ResDecoderBlock: 2-6                                  [1, 64, 320, 240]         258,880\n",
       "│    └─ResDecoderBlock: 2-7                                  [1, 32, 640, 480]         46,432\n",
       "├─Conv2d: 1-3                                                [1, 1, 640, 480]          33\n",
       "==============================================================================================================\n",
       "Total params: 356,105,601\n",
       "Trainable params: 356,105,601\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 136.37\n",
       "==============================================================================================================\n",
       "Input size (MB): 3.69\n",
       "Forward/backward pass size (MB): 2914.94\n",
       "Params size (MB): 1424.42\n",
       "Estimated Total Size (MB): 4343.05\n",
       "=============================================================================================================="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(net, input_size=(1, 3, 640, 480), depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c492935-2b7b-4f17-9c8f-ee04fbef1b00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
